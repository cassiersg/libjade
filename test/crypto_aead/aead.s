	.att_syntax
	.text
	.p2align	5
	.globl	_jade_aead_chacha20poly1305_amd64_ref_open
	.globl	jade_aead_chacha20poly1305_amd64_ref_open
	.globl	_jade_aead_chacha20poly1305_amd64_ref
	.globl	jade_aead_chacha20poly1305_amd64_ref
_jade_aead_chacha20poly1305_amd64_ref_open:
jade_aead_chacha20poly1305_amd64_ref_open:
	movq	%rsp, %rax
	leaq	-248(%rsp), %rsp
	andq	$-8, %rsp
	movq	%rax, 240(%rsp)
	movq	%rbx, 192(%rsp)
	movq	%rbp, 200(%rsp)
	movq	%r12, 208(%rsp)
	movq	%r13, 216(%rsp)
	movq	%r14, 224(%rsp)
	movq	%r15, 232(%rsp)
	movq	%rsi, %r10
	addq	%rdx, %r10
	movq	$-1, %rax
	cmpq	$16, %rcx
	jb  	Ljade_aead_chacha20poly1305_amd64_ref_open$1
	movq	%rdi, (%rsp)
	movq	%rsi, 8(%rsp)
	movq	%rdx, 16(%rsp)
	movq	%r10, 24(%rsp)
	leaq	16(%r10), %rax
	movq	%rax, 32(%rsp)
	leaq	-16(%rcx), %rax
	movq	%rax, 40(%rsp)
	movq	%r8, 48(%rsp)
	movq	%r9, 56(%rsp)
	movl	$0, %eax
	movl	$1634760805, 112(%rsp)
	movl	$857760878, 116(%rsp)
	movl	$2036477234, 120(%rsp)
	movl	$1797285236, 124(%rsp)
	movl	(%r9), %ecx
	movl	%ecx, 128(%rsp)
	movl	4(%r9), %ecx
	movl	%ecx, 132(%rsp)
	movl	8(%r9), %ecx
	movl	%ecx, 136(%rsp)
	movl	12(%r9), %ecx
	movl	%ecx, 140(%rsp)
	movl	16(%r9), %ecx
	movl	%ecx, 144(%rsp)
	movl	20(%r9), %ecx
	movl	%ecx, 148(%rsp)
	movl	24(%r9), %ecx
	movl	%ecx, 152(%rsp)
	movl	28(%r9), %ecx
	movl	%ecx, 156(%rsp)
	movl	%eax, 160(%rsp)
	movl	(%r8), %eax
	movl	%eax, 164(%rsp)
	movl	4(%r8), %eax
	movl	%eax, 168(%rsp)
	movl	8(%r8), %eax
	movl	%eax, 172(%rsp)
	movl	172(%rsp), %eax
	movl	%eax, 176(%rsp)
	movl	112(%rsp), %eax
	movl	116(%rsp), %ecx
	movl	120(%rsp), %edx
	movl	124(%rsp), %esi
	movl	128(%rsp), %edi
	movl	132(%rsp), %r8d
	movl	136(%rsp), %r9d
	movl	140(%rsp), %r10d
	movl	144(%rsp), %r11d
	movl	148(%rsp), %ebx
	movl	152(%rsp), %ebp
	movl	156(%rsp), %r12d
	movl	160(%rsp), %r13d
	movl	164(%rsp), %r14d
	movl	168(%rsp), %r15d
	movl	%r15d, 180(%rsp)
	movl	$10, %r15d
Ljade_aead_chacha20poly1305_amd64_ref_open$22:
	movl	%r15d, 184(%rsp)
	movl	180(%rsp), %r15d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$12, %edi
	roll	$12, %r9d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$7, %edi
	roll	$7, %r9d
	movl	%r15d, 188(%rsp)
	movl	176(%rsp), %r15d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$12, %r8d
	roll	$12, %r10d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$7, %r8d
	roll	$7, %r10d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$12, %r9d
	roll	$12, %r8d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$7, %r9d
	roll	$7, %r8d
	movl	%r15d, 176(%rsp)
	movl	188(%rsp), %r15d
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$12, %r10d
	roll	$12, %edi
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$7, %r10d
	roll	$7, %edi
	movl	%r15d, 180(%rsp)
	movl	184(%rsp), %r15d
	decl	%r15d
	cmpl	$0, %r15d
	jnbe	Ljade_aead_chacha20poly1305_amd64_ref_open$22
	addl	112(%rsp), %eax
	addl	116(%rsp), %ecx
	addl	120(%rsp), %edx
	addl	124(%rsp), %esi
	addl	128(%rsp), %edi
	addl	132(%rsp), %r8d
	addl	136(%rsp), %r9d
	addl	140(%rsp), %r10d
	movl	%eax, 80(%rsp)
	movl	%ecx, 84(%rsp)
	movl	%edx, 88(%rsp)
	movl	%esi, 92(%rsp)
	movl	%edi, 96(%rsp)
	movl	%r8d, 100(%rsp)
	movl	%r9d, 104(%rsp)
	movl	%r10d, 108(%rsp)
	movq	24(%rsp), %rcx
	movq	8(%rsp), %rsi
	movq	16(%rsp), %rdi
	movq	32(%rsp), %r8
	movq	40(%rsp), %rax
	movq	$0, %r9
	movq	$0, %r10
	movq	$0, %r11
	movq	80(%rsp), %rbx
	movq	88(%rsp), %rbp
	movq	$1152921487695413247, %rdx
	andq	%rdx, %rbx
	movq	$1152921487695413244, %rdx
	andq	%rdx, %rbp
	movq	%rbp, %r12
	shrq	$2, %r12
	addq	%rbp, %r12
	movq	%rdi, 16(%rsp)
	movq	%rax, 8(%rsp)
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref_open$20
Ljade_aead_chacha20poly1305_amd64_ref_open$21:
	addq	(%rsi), %r9
	adcq	8(%rsi), %r10
	adcq	$1, %r11
	movq	%r12, %r13
	imulq	%r11, %r13
	imulq	%rbx, %r11
	movq	%rbx, %rax
	mulq	%r9
	movq	%rax, %r14
	movq	%rdx, %r15
	movq	%rbx, %rax
	mulq	%r10
	addq	%rax, %r15
	adcq	%rdx, %r11
	movq	%r12, %rax
	mulq	%r10
	movq	%rdx, %r10
	addq	%r13, %r10
	movq	%rax, %r13
	movq	%rbp, %rax
	mulq	%r9
	addq	%r13, %r14
	adcq	%rax, %r15
	adcq	%rdx, %r11
	movq	$-4, %r9
	movq	%r11, %rax
	shrq	$2, %rax
	andq	%r11, %r9
	addq	%rax, %r9
	andq	$3, %r11
	addq	%r14, %r9
	adcq	%r15, %r10
	adcq	$0, %r11
	addq	$16, %rsi
	addq	$-16, %rdi
Ljade_aead_chacha20poly1305_amd64_ref_open$20:
	cmpq	$16, %rdi
	jnb 	Ljade_aead_chacha20poly1305_amd64_ref_open$21
	cmpq	$0, %rdi
	jbe 	Ljade_aead_chacha20poly1305_amd64_ref_open$17
	movq	$0, 64(%rsp)
	movq	$0, 72(%rsp)
	movq	$0, %rax
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref_open$18
Ljade_aead_chacha20poly1305_amd64_ref_open$19:
	movb	(%rsi,%rax), %dl
	movb	%dl, 64(%rsp,%rax)
	incq	%rax
Ljade_aead_chacha20poly1305_amd64_ref_open$18:
	cmpq	%rdi, %rax
	jb  	Ljade_aead_chacha20poly1305_amd64_ref_open$19
	addq	64(%rsp), %r9
	adcq	72(%rsp), %r10
	adcq	$1, %r11
	movq	%r12, %rsi
	imulq	%r11, %rsi
	imulq	%rbx, %r11
	movq	%rbx, %rax
	mulq	%r9
	movq	%rax, %rdi
	movq	%rdx, %r13
	movq	%rbx, %rax
	mulq	%r10
	addq	%rax, %r13
	adcq	%rdx, %r11
	movq	%r12, %rax
	mulq	%r10
	movq	%rdx, %r10
	addq	%rsi, %r10
	movq	%rax, %rsi
	movq	%rbp, %rax
	mulq	%r9
	addq	%rsi, %rdi
	adcq	%rax, %r13
	adcq	%rdx, %r11
	movq	$-4, %r9
	movq	%r11, %rax
	shrq	$2, %rax
	andq	%r11, %r9
	addq	%rax, %r9
	andq	$3, %r11
	addq	%rdi, %r9
	adcq	%r13, %r10
	adcq	$0, %r11
Ljade_aead_chacha20poly1305_amd64_ref_open$17:
	movq	8(%rsp), %rsi
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref_open$15
Ljade_aead_chacha20poly1305_amd64_ref_open$16:
	addq	(%r8), %r9
	adcq	8(%r8), %r10
	adcq	$1, %r11
	movq	%r12, %rdi
	imulq	%r11, %rdi
	imulq	%rbx, %r11
	movq	%rbx, %rax
	mulq	%r9
	movq	%rax, %r13
	movq	%rdx, %r14
	movq	%rbx, %rax
	mulq	%r10
	addq	%rax, %r14
	adcq	%rdx, %r11
	movq	%r12, %rax
	mulq	%r10
	movq	%rdx, %r10
	addq	%rdi, %r10
	movq	%rax, %rdi
	movq	%rbp, %rax
	mulq	%r9
	addq	%rdi, %r13
	adcq	%rax, %r14
	adcq	%rdx, %r11
	movq	$-4, %r9
	movq	%r11, %rax
	shrq	$2, %rax
	andq	%r11, %r9
	addq	%rax, %r9
	andq	$3, %r11
	addq	%r13, %r9
	adcq	%r14, %r10
	adcq	$0, %r11
	addq	$16, %r8
	addq	$-16, %rsi
Ljade_aead_chacha20poly1305_amd64_ref_open$15:
	cmpq	$16, %rsi
	jnb 	Ljade_aead_chacha20poly1305_amd64_ref_open$16
	cmpq	$0, %rsi
	jbe 	Ljade_aead_chacha20poly1305_amd64_ref_open$12
	movq	$0, 64(%rsp)
	movq	$0, 72(%rsp)
	movq	$0, %rax
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref_open$13
Ljade_aead_chacha20poly1305_amd64_ref_open$14:
	movb	(%r8,%rax), %dl
	movb	%dl, 64(%rsp,%rax)
	incq	%rax
Ljade_aead_chacha20poly1305_amd64_ref_open$13:
	cmpq	%rsi, %rax
	jb  	Ljade_aead_chacha20poly1305_amd64_ref_open$14
	addq	64(%rsp), %r9
	adcq	72(%rsp), %r10
	adcq	$1, %r11
	movq	%r12, %rsi
	imulq	%r11, %rsi
	imulq	%rbx, %r11
	movq	%rbx, %rax
	mulq	%r9
	movq	%rax, %rdi
	movq	%rdx, %r8
	movq	%rbx, %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r11
	movq	%r12, %rax
	mulq	%r10
	movq	%rdx, %r10
	addq	%rsi, %r10
	movq	%rax, %rsi
	movq	%rbp, %rax
	mulq	%r9
	addq	%rsi, %rdi
	adcq	%rax, %r8
	adcq	%rdx, %r11
	movq	$-4, %r9
	movq	%r11, %rax
	shrq	$2, %rax
	andq	%r11, %r9
	addq	%rax, %r9
	andq	$3, %r11
	addq	%rdi, %r9
	adcq	%r8, %r10
	adcq	$0, %r11
Ljade_aead_chacha20poly1305_amd64_ref_open$12:
	movq	16(%rsp), %rax
	movq	8(%rsp), %rdx
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$1, %r11
	movq	%r12, %rsi
	imulq	%r11, %rsi
	imulq	%rbx, %r11
	movq	%rbx, %rax
	mulq	%r9
	movq	%rax, %rdi
	movq	%rdx, %r8
	movq	%rbx, %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r11
	movq	%r12, %rax
	mulq	%r10
	movq	%rdx, %r10
	addq	%rsi, %r10
	movq	%rax, %rsi
	movq	%rbp, %rax
	mulq	%r9
	addq	%rsi, %rdi
	adcq	%rax, %r8
	adcq	%rdx, %r11
	movq	$-4, %rax
	movq	%r11, %rdx
	shrq	$2, %rdx
	andq	%r11, %rax
	addq	%rdx, %rax
	andq	$3, %r11
	addq	%rdi, %rax
	adcq	%r8, %r10
	adcq	$0, %r11
	movq	%rax, %rdx
	movq	%r10, %rsi
	addq	$5, %rdx
	adcq	$0, %rsi
	adcq	$0, %r11
	shrq	$2, %r11
	negq	%r11
	xorq	%rax, %rdx
	xorq	%r10, %rsi
	andq	%r11, %rdx
	andq	%r11, %rsi
	xorq	%rax, %rdx
	xorq	%r10, %rsi
	movq	96(%rsp), %rax
	movq	104(%rsp), %rdi
	addq	%rax, %rdx
	adcq	%rdi, %rsi
	xorq	(%rcx), %rdx
	xorq	8(%rcx), %rsi
	orq 	%rsi, %rdx
	xorq	%rax, %rax
	subq	$1, %rdx
	adcq	$0, %rax
	addq	$-1, %rax
	cmpq	$0, %rax
	jne 	Ljade_aead_chacha20poly1305_amd64_ref_open$1
	movq	(%rsp), %rax
	movq	32(%rsp), %rcx
	movq	40(%rsp), %rdx
	movq	48(%rsp), %rsi
	movq	56(%rsp), %rdi
	movq	%rax, 56(%rsp)
	movq	%rcx, 48(%rsp)
	movq	%rdx, 40(%rsp)
	movl	$1, %eax
	movl	$1634760805, 112(%rsp)
	movl	$857760878, 116(%rsp)
	movl	$2036477234, 120(%rsp)
	movl	$1797285236, 124(%rsp)
	movl	(%rdi), %ecx
	movl	%ecx, 128(%rsp)
	movl	4(%rdi), %ecx
	movl	%ecx, 132(%rsp)
	movl	8(%rdi), %ecx
	movl	%ecx, 136(%rsp)
	movl	12(%rdi), %ecx
	movl	%ecx, 140(%rsp)
	movl	16(%rdi), %ecx
	movl	%ecx, 144(%rsp)
	movl	20(%rdi), %ecx
	movl	%ecx, 148(%rsp)
	movl	24(%rdi), %ecx
	movl	%ecx, 152(%rsp)
	movl	28(%rdi), %ecx
	movl	%ecx, 156(%rsp)
	movl	%eax, 160(%rsp)
	movl	(%rsi), %eax
	movl	%eax, 164(%rsp)
	movl	4(%rsi), %eax
	movl	%eax, 168(%rsp)
	movl	8(%rsi), %eax
	movl	%eax, 172(%rsp)
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref_open$9
Ljade_aead_chacha20poly1305_amd64_ref_open$10:
	movl	172(%rsp), %eax
	movl	%eax, 180(%rsp)
	movl	112(%rsp), %eax
	movl	116(%rsp), %ecx
	movl	120(%rsp), %edx
	movl	124(%rsp), %esi
	movl	128(%rsp), %edi
	movl	132(%rsp), %r8d
	movl	136(%rsp), %r9d
	movl	140(%rsp), %r10d
	movl	144(%rsp), %r11d
	movl	148(%rsp), %ebx
	movl	152(%rsp), %ebp
	movl	156(%rsp), %r12d
	movl	160(%rsp), %r13d
	movl	164(%rsp), %r14d
	movl	168(%rsp), %r15d
	movl	%r15d, 176(%rsp)
	movl	$10, %r15d
Ljade_aead_chacha20poly1305_amd64_ref_open$11:
	movl	%r15d, 184(%rsp)
	movl	176(%rsp), %r15d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$12, %edi
	roll	$12, %r9d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$7, %edi
	roll	$7, %r9d
	movl	%r15d, 188(%rsp)
	movl	180(%rsp), %r15d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$12, %r8d
	roll	$12, %r10d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$7, %r8d
	roll	$7, %r10d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$12, %r9d
	roll	$12, %r8d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$7, %r9d
	roll	$7, %r8d
	movl	%r15d, 180(%rsp)
	movl	188(%rsp), %r15d
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$12, %r10d
	roll	$12, %edi
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$7, %r10d
	roll	$7, %edi
	movl	%r15d, 176(%rsp)
	movl	184(%rsp), %r15d
	decl	%r15d
	cmpl	$0, %r15d
	jnbe	Ljade_aead_chacha20poly1305_amd64_ref_open$11
	movl	176(%rsp), %r15d
	addl	116(%rsp), %ecx
	addl	112(%rsp), %eax
	movl	%ecx, %ecx
	shlq	$32, %rcx
	movl	%eax, %eax
	xorq	%rax, %rcx
	movq	48(%rsp), %rax
	xorq	(%rax), %rcx
	addl	124(%rsp), %esi
	addl	120(%rsp), %edx
	movl	%esi, %esi
	shlq	$32, %rsi
	movl	%edx, %edx
	xorq	%rdx, %rsi
	xorq	8(%rax), %rsi
	movq	56(%rsp), %rdx
	movq	%rcx, (%rdx)
	addl	132(%rsp), %r8d
	addl	128(%rsp), %edi
	movl	%r8d, %ecx
	shlq	$32, %rcx
	movl	%edi, %edi
	xorq	%rdi, %rcx
	xorq	16(%rax), %rcx
	movq	%rsi, 8(%rdx)
	addl	140(%rsp), %r10d
	addl	136(%rsp), %r9d
	movl	%r10d, %esi
	shlq	$32, %rsi
	movl	%r9d, %edi
	xorq	%rdi, %rsi
	xorq	24(%rax), %rsi
	movq	%rcx, 16(%rdx)
	addl	148(%rsp), %ebx
	addl	144(%rsp), %r11d
	movl	%ebx, %ecx
	shlq	$32, %rcx
	movl	%r11d, %edi
	xorq	%rdi, %rcx
	xorq	32(%rax), %rcx
	movq	%rsi, 24(%rdx)
	addl	156(%rsp), %r12d
	addl	152(%rsp), %ebp
	movl	%r12d, %esi
	shlq	$32, %rsi
	movl	%ebp, %edi
	xorq	%rdi, %rsi
	xorq	40(%rax), %rsi
	movq	%rcx, 32(%rdx)
	addl	164(%rsp), %r14d
	addl	160(%rsp), %r13d
	movl	%r14d, %ecx
	shlq	$32, %rcx
	movl	%r13d, %edi
	xorq	%rdi, %rcx
	xorq	48(%rax), %rcx
	movq	%rsi, 40(%rdx)
	movl	180(%rsp), %esi
	addl	172(%rsp), %esi
	addl	168(%rsp), %r15d
	movl	%esi, %esi
	shlq	$32, %rsi
	movl	%r15d, %edi
	xorq	%rdi, %rsi
	xorq	56(%rax), %rsi
	movq	%rcx, 48(%rdx)
	movq	%rsi, 56(%rdx)
	movq	40(%rsp), %rcx
	addq	$64, %rdx
	addq	$64, %rax
	addq	$-64, %rcx
	movq	%rdx, 56(%rsp)
	movq	%rax, 48(%rsp)
	movq	%rcx, 40(%rsp)
	movl	160(%rsp), %eax
	incl	%eax
	movl	%eax, 160(%rsp)
Ljade_aead_chacha20poly1305_amd64_ref_open$9:
	movq	40(%rsp), %rax
	cmpq	$64, %rax
	jnb 	Ljade_aead_chacha20poly1305_amd64_ref_open$10
	cmpq	$0, %rax
	jbe 	Ljade_aead_chacha20poly1305_amd64_ref_open$3
	movl	172(%rsp), %eax
	movl	%eax, 180(%rsp)
	movl	112(%rsp), %eax
	movl	116(%rsp), %ecx
	movl	120(%rsp), %edx
	movl	124(%rsp), %esi
	movl	128(%rsp), %edi
	movl	132(%rsp), %r8d
	movl	136(%rsp), %r9d
	movl	140(%rsp), %r10d
	movl	144(%rsp), %r11d
	movl	148(%rsp), %ebx
	movl	152(%rsp), %ebp
	movl	156(%rsp), %r12d
	movl	160(%rsp), %r13d
	movl	164(%rsp), %r14d
	movl	168(%rsp), %r15d
	movl	%r15d, 176(%rsp)
	movl	$10, %r15d
Ljade_aead_chacha20poly1305_amd64_ref_open$8:
	movl	%r15d, 184(%rsp)
	movl	176(%rsp), %r15d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$12, %edi
	roll	$12, %r9d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$7, %edi
	roll	$7, %r9d
	movl	%r15d, 188(%rsp)
	movl	180(%rsp), %r15d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$12, %r8d
	roll	$12, %r10d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$7, %r8d
	roll	$7, %r10d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$12, %r9d
	roll	$12, %r8d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$7, %r9d
	roll	$7, %r8d
	movl	%r15d, 180(%rsp)
	movl	188(%rsp), %r15d
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$12, %r10d
	roll	$12, %edi
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$7, %r10d
	roll	$7, %edi
	movl	%r15d, 176(%rsp)
	movl	184(%rsp), %r15d
	decl	%r15d
	cmpl	$0, %r15d
	jnbe	Ljade_aead_chacha20poly1305_amd64_ref_open$8
	movl	176(%rsp), %r15d
	addl	112(%rsp), %eax
	addl	116(%rsp), %ecx
	addl	120(%rsp), %edx
	addl	124(%rsp), %esi
	addl	128(%rsp), %edi
	addl	132(%rsp), %r8d
	addl	136(%rsp), %r9d
	addl	140(%rsp), %r10d
	addl	144(%rsp), %r11d
	addl	148(%rsp), %ebx
	addl	152(%rsp), %ebp
	addl	156(%rsp), %r12d
	addl	160(%rsp), %r13d
	addl	164(%rsp), %r14d
	addl	168(%rsp), %r15d
	movl	%r15d, 176(%rsp)
	movl	180(%rsp), %r15d
	addl	172(%rsp), %r15d
	movl	%r15d, 180(%rsp)
	movl	176(%rsp), %r15d
	movl	%eax, 112(%rsp)
	movl	%ecx, 116(%rsp)
	movl	%edx, 120(%rsp)
	movl	%esi, 124(%rsp)
	movl	%edi, 128(%rsp)
	movl	%r8d, 132(%rsp)
	movl	%r9d, 136(%rsp)
	movl	%r10d, 140(%rsp)
	movl	%r11d, 144(%rsp)
	movl	%ebx, 148(%rsp)
	movl	%ebp, 152(%rsp)
	movl	%r12d, 156(%rsp)
	movl	%r13d, 160(%rsp)
	movl	%r14d, 164(%rsp)
	movl	%r15d, 168(%rsp)
	movl	180(%rsp), %eax
	movl	%eax, 172(%rsp)
	movq	56(%rsp), %rax
	movq	48(%rsp), %rcx
	movq	40(%rsp), %rdx
	movq	%rdx, %rsi
	shrq	$3, %rsi
	movq	$0, %rdi
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref_open$6
Ljade_aead_chacha20poly1305_amd64_ref_open$7:
	movq	(%rcx,%rdi,8), %r8
	xorq	112(%rsp,%rdi,8), %r8
	movq	%r8, (%rax,%rdi,8)
	incq	%rdi
Ljade_aead_chacha20poly1305_amd64_ref_open$6:
	cmpq	%rsi, %rdi
	jb  	Ljade_aead_chacha20poly1305_amd64_ref_open$7
	shlq	$3, %rdi
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref_open$4
Ljade_aead_chacha20poly1305_amd64_ref_open$5:
	movb	(%rcx,%rdi), %sil
	xorb	112(%rsp,%rdi), %sil
	movb	%sil, (%rax,%rdi)
	incq	%rdi
Ljade_aead_chacha20poly1305_amd64_ref_open$4:
	cmpq	%rdx, %rdi
	jb  	Ljade_aead_chacha20poly1305_amd64_ref_open$5
Ljade_aead_chacha20poly1305_amd64_ref_open$3:
	movq	$0, %rax
Ljade_aead_chacha20poly1305_amd64_ref_open$2:
Ljade_aead_chacha20poly1305_amd64_ref_open$1:
	movq	192(%rsp), %rbx
	movq	200(%rsp), %rbp
	movq	208(%rsp), %r12
	movq	216(%rsp), %r13
	movq	224(%rsp), %r14
	movq	232(%rsp), %r15
	movq	240(%rsp), %rsp
	ret 
_jade_aead_chacha20poly1305_amd64_ref:
jade_aead_chacha20poly1305_amd64_ref:
	movq	%rsp, %rax
	leaq	-264(%rsp), %rsp
	andq	$-8, %rsp
	movq	%rax, 256(%rsp)
	movq	%rbx, 208(%rsp)
	movq	%rbp, 216(%rsp)
	movq	%r12, 224(%rsp)
	movq	%r13, 232(%rsp)
	movq	%r14, 240(%rsp)
	movq	%r15, 248(%rsp)
	movq	%rsi, %rax
	addq	%rdx, %rax
	movq	%rdi, (%rsp)
	leaq	16(%rdi), %rdi
	movq	%rdi, 8(%rsp)
	movq	%rsi, 16(%rsp)
	movq	%rdx, 24(%rsp)
	movq	%rcx, 32(%rsp)
	movq	%r8, 40(%rsp)
	movq	%r9, 48(%rsp)
	movq	%rdi, 56(%rsp)
	movq	%rax, 64(%rsp)
	movq	%rcx, 72(%rsp)
	movl	$1, %eax
	movl	$1634760805, 128(%rsp)
	movl	$857760878, 132(%rsp)
	movl	$2036477234, 136(%rsp)
	movl	$1797285236, 140(%rsp)
	movl	(%r9), %ecx
	movl	%ecx, 144(%rsp)
	movl	4(%r9), %ecx
	movl	%ecx, 148(%rsp)
	movl	8(%r9), %ecx
	movl	%ecx, 152(%rsp)
	movl	12(%r9), %ecx
	movl	%ecx, 156(%rsp)
	movl	16(%r9), %ecx
	movl	%ecx, 160(%rsp)
	movl	20(%r9), %ecx
	movl	%ecx, 164(%rsp)
	movl	24(%r9), %ecx
	movl	%ecx, 168(%rsp)
	movl	28(%r9), %ecx
	movl	%ecx, 172(%rsp)
	movl	%eax, 176(%rsp)
	movl	(%r8), %eax
	movl	%eax, 180(%rsp)
	movl	4(%r8), %eax
	movl	%eax, 184(%rsp)
	movl	8(%r8), %eax
	movl	%eax, 188(%rsp)
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref$18
Ljade_aead_chacha20poly1305_amd64_ref$19:
	movl	188(%rsp), %eax
	movl	%eax, 192(%rsp)
	movl	128(%rsp), %eax
	movl	132(%rsp), %ecx
	movl	136(%rsp), %edx
	movl	140(%rsp), %esi
	movl	144(%rsp), %edi
	movl	148(%rsp), %r8d
	movl	152(%rsp), %r9d
	movl	156(%rsp), %r10d
	movl	160(%rsp), %r11d
	movl	164(%rsp), %ebx
	movl	168(%rsp), %ebp
	movl	172(%rsp), %r12d
	movl	176(%rsp), %r13d
	movl	180(%rsp), %r14d
	movl	184(%rsp), %r15d
	movl	%r15d, 196(%rsp)
	movl	$10, %r15d
Ljade_aead_chacha20poly1305_amd64_ref$20:
	movl	%r15d, 200(%rsp)
	movl	196(%rsp), %r15d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$12, %edi
	roll	$12, %r9d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$7, %edi
	roll	$7, %r9d
	movl	%r15d, 204(%rsp)
	movl	192(%rsp), %r15d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$12, %r8d
	roll	$12, %r10d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$7, %r8d
	roll	$7, %r10d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$12, %r9d
	roll	$12, %r8d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$7, %r9d
	roll	$7, %r8d
	movl	%r15d, 192(%rsp)
	movl	204(%rsp), %r15d
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$12, %r10d
	roll	$12, %edi
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$7, %r10d
	roll	$7, %edi
	movl	%r15d, 196(%rsp)
	movl	200(%rsp), %r15d
	decl	%r15d
	cmpl	$0, %r15d
	jnbe	Ljade_aead_chacha20poly1305_amd64_ref$20
	movl	196(%rsp), %r15d
	addl	132(%rsp), %ecx
	addl	128(%rsp), %eax
	movl	%ecx, %ecx
	shlq	$32, %rcx
	movl	%eax, %eax
	xorq	%rax, %rcx
	movq	64(%rsp), %rax
	xorq	(%rax), %rcx
	addl	140(%rsp), %esi
	addl	136(%rsp), %edx
	movl	%esi, %esi
	shlq	$32, %rsi
	movl	%edx, %edx
	xorq	%rdx, %rsi
	xorq	8(%rax), %rsi
	movq	56(%rsp), %rdx
	movq	%rcx, (%rdx)
	addl	148(%rsp), %r8d
	addl	144(%rsp), %edi
	movl	%r8d, %ecx
	shlq	$32, %rcx
	movl	%edi, %edi
	xorq	%rdi, %rcx
	xorq	16(%rax), %rcx
	movq	%rsi, 8(%rdx)
	addl	156(%rsp), %r10d
	addl	152(%rsp), %r9d
	movl	%r10d, %esi
	shlq	$32, %rsi
	movl	%r9d, %edi
	xorq	%rdi, %rsi
	xorq	24(%rax), %rsi
	movq	%rcx, 16(%rdx)
	addl	164(%rsp), %ebx
	addl	160(%rsp), %r11d
	movl	%ebx, %ecx
	shlq	$32, %rcx
	movl	%r11d, %edi
	xorq	%rdi, %rcx
	xorq	32(%rax), %rcx
	movq	%rsi, 24(%rdx)
	addl	172(%rsp), %r12d
	addl	168(%rsp), %ebp
	movl	%r12d, %esi
	shlq	$32, %rsi
	movl	%ebp, %edi
	xorq	%rdi, %rsi
	xorq	40(%rax), %rsi
	movq	%rcx, 32(%rdx)
	addl	180(%rsp), %r14d
	addl	176(%rsp), %r13d
	movl	%r14d, %ecx
	shlq	$32, %rcx
	movl	%r13d, %edi
	xorq	%rdi, %rcx
	xorq	48(%rax), %rcx
	movq	%rsi, 40(%rdx)
	movl	192(%rsp), %esi
	addl	188(%rsp), %esi
	addl	184(%rsp), %r15d
	movl	%esi, %esi
	shlq	$32, %rsi
	movl	%r15d, %edi
	xorq	%rdi, %rsi
	xorq	56(%rax), %rsi
	movq	%rcx, 48(%rdx)
	movq	%rsi, 56(%rdx)
	movq	72(%rsp), %rcx
	addq	$64, %rdx
	addq	$64, %rax
	addq	$-64, %rcx
	movq	%rdx, 56(%rsp)
	movq	%rax, 64(%rsp)
	movq	%rcx, 72(%rsp)
	movl	176(%rsp), %eax
	incl	%eax
	movl	%eax, 176(%rsp)
Ljade_aead_chacha20poly1305_amd64_ref$18:
	movq	72(%rsp), %rax
	cmpq	$64, %rax
	jnb 	Ljade_aead_chacha20poly1305_amd64_ref$19
	cmpq	$0, %rax
	jbe 	Ljade_aead_chacha20poly1305_amd64_ref$12
	movl	188(%rsp), %eax
	movl	%eax, 192(%rsp)
	movl	128(%rsp), %eax
	movl	132(%rsp), %ecx
	movl	136(%rsp), %edx
	movl	140(%rsp), %esi
	movl	144(%rsp), %edi
	movl	148(%rsp), %r8d
	movl	152(%rsp), %r9d
	movl	156(%rsp), %r10d
	movl	160(%rsp), %r11d
	movl	164(%rsp), %ebx
	movl	168(%rsp), %ebp
	movl	172(%rsp), %r12d
	movl	176(%rsp), %r13d
	movl	180(%rsp), %r14d
	movl	184(%rsp), %r15d
	movl	%r15d, 196(%rsp)
	movl	$10, %r15d
Ljade_aead_chacha20poly1305_amd64_ref$17:
	movl	%r15d, 200(%rsp)
	movl	196(%rsp), %r15d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$12, %edi
	roll	$12, %r9d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$7, %edi
	roll	$7, %r9d
	movl	%r15d, 204(%rsp)
	movl	192(%rsp), %r15d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$12, %r8d
	roll	$12, %r10d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$7, %r8d
	roll	$7, %r10d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$12, %r9d
	roll	$12, %r8d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$7, %r9d
	roll	$7, %r8d
	movl	%r15d, 192(%rsp)
	movl	204(%rsp), %r15d
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$12, %r10d
	roll	$12, %edi
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$7, %r10d
	roll	$7, %edi
	movl	%r15d, 196(%rsp)
	movl	200(%rsp), %r15d
	decl	%r15d
	cmpl	$0, %r15d
	jnbe	Ljade_aead_chacha20poly1305_amd64_ref$17
	movl	196(%rsp), %r15d
	addl	128(%rsp), %eax
	addl	132(%rsp), %ecx
	addl	136(%rsp), %edx
	addl	140(%rsp), %esi
	addl	144(%rsp), %edi
	addl	148(%rsp), %r8d
	addl	152(%rsp), %r9d
	addl	156(%rsp), %r10d
	addl	160(%rsp), %r11d
	addl	164(%rsp), %ebx
	addl	168(%rsp), %ebp
	addl	172(%rsp), %r12d
	addl	176(%rsp), %r13d
	addl	180(%rsp), %r14d
	addl	184(%rsp), %r15d
	movl	%r15d, 196(%rsp)
	movl	192(%rsp), %r15d
	addl	188(%rsp), %r15d
	movl	%r15d, 192(%rsp)
	movl	196(%rsp), %r15d
	movl	%eax, 128(%rsp)
	movl	%ecx, 132(%rsp)
	movl	%edx, 136(%rsp)
	movl	%esi, 140(%rsp)
	movl	%edi, 144(%rsp)
	movl	%r8d, 148(%rsp)
	movl	%r9d, 152(%rsp)
	movl	%r10d, 156(%rsp)
	movl	%r11d, 160(%rsp)
	movl	%ebx, 164(%rsp)
	movl	%ebp, 168(%rsp)
	movl	%r12d, 172(%rsp)
	movl	%r13d, 176(%rsp)
	movl	%r14d, 180(%rsp)
	movl	%r15d, 184(%rsp)
	movl	192(%rsp), %eax
	movl	%eax, 188(%rsp)
	movq	56(%rsp), %rax
	movq	64(%rsp), %rcx
	movq	72(%rsp), %rdx
	movq	%rdx, %rsi
	shrq	$3, %rsi
	movq	$0, %rdi
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref$15
Ljade_aead_chacha20poly1305_amd64_ref$16:
	movq	(%rcx,%rdi,8), %r8
	xorq	128(%rsp,%rdi,8), %r8
	movq	%r8, (%rax,%rdi,8)
	incq	%rdi
Ljade_aead_chacha20poly1305_amd64_ref$15:
	cmpq	%rsi, %rdi
	jb  	Ljade_aead_chacha20poly1305_amd64_ref$16
	shlq	$3, %rdi
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref$13
Ljade_aead_chacha20poly1305_amd64_ref$14:
	movb	(%rcx,%rdi), %sil
	xorb	128(%rsp,%rdi), %sil
	movb	%sil, (%rax,%rdi)
	incq	%rdi
Ljade_aead_chacha20poly1305_amd64_ref$13:
	cmpq	%rdx, %rdi
	jb  	Ljade_aead_chacha20poly1305_amd64_ref$14
Ljade_aead_chacha20poly1305_amd64_ref$12:
	movq	40(%rsp), %rax
	movq	48(%rsp), %rcx
	movl	$0, %edx
	movl	$1634760805, 128(%rsp)
	movl	$857760878, 132(%rsp)
	movl	$2036477234, 136(%rsp)
	movl	$1797285236, 140(%rsp)
	movl	(%rcx), %esi
	movl	%esi, 144(%rsp)
	movl	4(%rcx), %esi
	movl	%esi, 148(%rsp)
	movl	8(%rcx), %esi
	movl	%esi, 152(%rsp)
	movl	12(%rcx), %esi
	movl	%esi, 156(%rsp)
	movl	16(%rcx), %esi
	movl	%esi, 160(%rsp)
	movl	20(%rcx), %esi
	movl	%esi, 164(%rsp)
	movl	24(%rcx), %esi
	movl	%esi, 168(%rsp)
	movl	28(%rcx), %ecx
	movl	%ecx, 172(%rsp)
	movl	%edx, 176(%rsp)
	movl	(%rax), %ecx
	movl	%ecx, 180(%rsp)
	movl	4(%rax), %ecx
	movl	%ecx, 184(%rsp)
	movl	8(%rax), %eax
	movl	%eax, 188(%rsp)
	movl	188(%rsp), %eax
	movl	%eax, 192(%rsp)
	movl	128(%rsp), %eax
	movl	132(%rsp), %ecx
	movl	136(%rsp), %edx
	movl	140(%rsp), %esi
	movl	144(%rsp), %edi
	movl	148(%rsp), %r8d
	movl	152(%rsp), %r9d
	movl	156(%rsp), %r10d
	movl	160(%rsp), %r11d
	movl	164(%rsp), %ebx
	movl	168(%rsp), %ebp
	movl	172(%rsp), %r12d
	movl	176(%rsp), %r13d
	movl	180(%rsp), %r14d
	movl	184(%rsp), %r15d
	movl	%r15d, 196(%rsp)
	movl	$10, %r15d
Ljade_aead_chacha20poly1305_amd64_ref$11:
	movl	%r15d, 200(%rsp)
	movl	196(%rsp), %r15d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$12, %edi
	roll	$12, %r9d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$7, %edi
	roll	$7, %r9d
	movl	%r15d, 204(%rsp)
	movl	192(%rsp), %r15d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$12, %r8d
	roll	$12, %r10d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$7, %r8d
	roll	$7, %r10d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$12, %r9d
	roll	$12, %r8d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$7, %r9d
	roll	$7, %r8d
	movl	%r15d, 192(%rsp)
	movl	204(%rsp), %r15d
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$12, %r10d
	roll	$12, %edi
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$7, %r10d
	roll	$7, %edi
	movl	%r15d, 196(%rsp)
	movl	200(%rsp), %r15d
	decl	%r15d
	cmpl	$0, %r15d
	jnbe	Ljade_aead_chacha20poly1305_amd64_ref$11
	addl	128(%rsp), %eax
	addl	132(%rsp), %ecx
	addl	136(%rsp), %edx
	addl	140(%rsp), %esi
	addl	144(%rsp), %edi
	addl	148(%rsp), %r8d
	addl	152(%rsp), %r9d
	addl	156(%rsp), %r10d
	movl	%eax, 96(%rsp)
	movl	%ecx, 100(%rsp)
	movl	%edx, 104(%rsp)
	movl	%esi, 108(%rsp)
	movl	%edi, 112(%rsp)
	movl	%r8d, 116(%rsp)
	movl	%r9d, 120(%rsp)
	movl	%r10d, 124(%rsp)
	movq	(%rsp), %rcx
	movq	8(%rsp), %rsi
	movq	16(%rsp), %rdi
	movq	24(%rsp), %r8
	movq	32(%rsp), %rax
	movq	$0, %r9
	movq	$0, %r10
	movq	$0, %r11
	movq	96(%rsp), %rbx
	movq	104(%rsp), %rbp
	movq	$1152921487695413247, %rdx
	andq	%rdx, %rbx
	movq	$1152921487695413244, %rdx
	andq	%rdx, %rbp
	movq	%rbp, %r12
	shrq	$2, %r12
	addq	%rbp, %r12
	movq	%r8, 32(%rsp)
	movq	%rax, 24(%rsp)
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref$9
Ljade_aead_chacha20poly1305_amd64_ref$10:
	addq	(%rdi), %r9
	adcq	8(%rdi), %r10
	adcq	$1, %r11
	movq	%r12, %r13
	imulq	%r11, %r13
	imulq	%rbx, %r11
	movq	%rbx, %rax
	mulq	%r9
	movq	%rax, %r14
	movq	%rdx, %r15
	movq	%rbx, %rax
	mulq	%r10
	addq	%rax, %r15
	adcq	%rdx, %r11
	movq	%r12, %rax
	mulq	%r10
	movq	%rdx, %r10
	addq	%r13, %r10
	movq	%rax, %r13
	movq	%rbp, %rax
	mulq	%r9
	addq	%r13, %r14
	adcq	%rax, %r15
	adcq	%rdx, %r11
	movq	$-4, %r9
	movq	%r11, %rax
	shrq	$2, %rax
	andq	%r11, %r9
	addq	%rax, %r9
	andq	$3, %r11
	addq	%r14, %r9
	adcq	%r15, %r10
	adcq	$0, %r11
	addq	$16, %rdi
	addq	$-16, %r8
Ljade_aead_chacha20poly1305_amd64_ref$9:
	cmpq	$16, %r8
	jnb 	Ljade_aead_chacha20poly1305_amd64_ref$10
	cmpq	$0, %r8
	jbe 	Ljade_aead_chacha20poly1305_amd64_ref$6
	movq	$0, 80(%rsp)
	movq	$0, 88(%rsp)
	movq	$0, %rax
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref$7
Ljade_aead_chacha20poly1305_amd64_ref$8:
	movb	(%rdi,%rax), %dl
	movb	%dl, 80(%rsp,%rax)
	incq	%rax
Ljade_aead_chacha20poly1305_amd64_ref$7:
	cmpq	%r8, %rax
	jb  	Ljade_aead_chacha20poly1305_amd64_ref$8
	addq	80(%rsp), %r9
	adcq	88(%rsp), %r10
	adcq	$1, %r11
	movq	%r12, %rdi
	imulq	%r11, %rdi
	imulq	%rbx, %r11
	movq	%rbx, %rax
	mulq	%r9
	movq	%rax, %r8
	movq	%rdx, %r13
	movq	%rbx, %rax
	mulq	%r10
	addq	%rax, %r13
	adcq	%rdx, %r11
	movq	%r12, %rax
	mulq	%r10
	movq	%rdx, %r10
	addq	%rdi, %r10
	movq	%rax, %rdi
	movq	%rbp, %rax
	mulq	%r9
	addq	%rdi, %r8
	adcq	%rax, %r13
	adcq	%rdx, %r11
	movq	$-4, %r9
	movq	%r11, %rax
	shrq	$2, %rax
	andq	%r11, %r9
	addq	%rax, %r9
	andq	$3, %r11
	addq	%r8, %r9
	adcq	%r13, %r10
	adcq	$0, %r11
Ljade_aead_chacha20poly1305_amd64_ref$6:
	movq	24(%rsp), %rdi
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref$4
Ljade_aead_chacha20poly1305_amd64_ref$5:
	addq	(%rsi), %r9
	adcq	8(%rsi), %r10
	adcq	$1, %r11
	movq	%r12, %r8
	imulq	%r11, %r8
	imulq	%rbx, %r11
	movq	%rbx, %rax
	mulq	%r9
	movq	%rax, %r13
	movq	%rdx, %r14
	movq	%rbx, %rax
	mulq	%r10
	addq	%rax, %r14
	adcq	%rdx, %r11
	movq	%r12, %rax
	mulq	%r10
	movq	%rdx, %r10
	addq	%r8, %r10
	movq	%rax, %r8
	movq	%rbp, %rax
	mulq	%r9
	addq	%r8, %r13
	adcq	%rax, %r14
	adcq	%rdx, %r11
	movq	$-4, %r9
	movq	%r11, %rax
	shrq	$2, %rax
	andq	%r11, %r9
	addq	%rax, %r9
	andq	$3, %r11
	addq	%r13, %r9
	adcq	%r14, %r10
	adcq	$0, %r11
	addq	$16, %rsi
	addq	$-16, %rdi
Ljade_aead_chacha20poly1305_amd64_ref$4:
	cmpq	$16, %rdi
	jnb 	Ljade_aead_chacha20poly1305_amd64_ref$5
	cmpq	$0, %rdi
	jbe 	Ljade_aead_chacha20poly1305_amd64_ref$1
	movq	$0, 80(%rsp)
	movq	$0, 88(%rsp)
	movq	$0, %rax
	jmp 	Ljade_aead_chacha20poly1305_amd64_ref$2
Ljade_aead_chacha20poly1305_amd64_ref$3:
	movb	(%rsi,%rax), %dl
	movb	%dl, 80(%rsp,%rax)
	incq	%rax
Ljade_aead_chacha20poly1305_amd64_ref$2:
	cmpq	%rdi, %rax
	jb  	Ljade_aead_chacha20poly1305_amd64_ref$3
	addq	80(%rsp), %r9
	adcq	88(%rsp), %r10
	adcq	$1, %r11
	movq	%r12, %rsi
	imulq	%r11, %rsi
	imulq	%rbx, %r11
	movq	%rbx, %rax
	mulq	%r9
	movq	%rax, %rdi
	movq	%rdx, %r8
	movq	%rbx, %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r11
	movq	%r12, %rax
	mulq	%r10
	movq	%rdx, %r10
	addq	%rsi, %r10
	movq	%rax, %rsi
	movq	%rbp, %rax
	mulq	%r9
	addq	%rsi, %rdi
	adcq	%rax, %r8
	adcq	%rdx, %r11
	movq	$-4, %r9
	movq	%r11, %rax
	shrq	$2, %rax
	andq	%r11, %r9
	addq	%rax, %r9
	andq	$3, %r11
	addq	%rdi, %r9
	adcq	%r8, %r10
	adcq	$0, %r11
Ljade_aead_chacha20poly1305_amd64_ref$1:
	movq	32(%rsp), %rax
	movq	24(%rsp), %rdx
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$1, %r11
	movq	%r12, %rsi
	imulq	%r11, %rsi
	imulq	%rbx, %r11
	movq	%rbx, %rax
	mulq	%r9
	movq	%rax, %rdi
	movq	%rdx, %r8
	movq	%rbx, %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r11
	movq	%r12, %rax
	mulq	%r10
	movq	%rdx, %r10
	addq	%rsi, %r10
	movq	%rax, %rsi
	movq	%rbp, %rax
	mulq	%r9
	addq	%rsi, %rdi
	adcq	%rax, %r8
	adcq	%rdx, %r11
	movq	$-4, %rax
	movq	%r11, %rdx
	shrq	$2, %rdx
	andq	%r11, %rax
	addq	%rdx, %rax
	andq	$3, %r11
	addq	%rdi, %rax
	adcq	%r8, %r10
	adcq	$0, %r11
	movq	%rax, %rdx
	movq	%r10, %rsi
	addq	$5, %rdx
	adcq	$0, %rsi
	adcq	$0, %r11
	shrq	$2, %r11
	negq	%r11
	xorq	%rax, %rdx
	xorq	%r10, %rsi
	andq	%r11, %rdx
	andq	%r11, %rsi
	xorq	%rax, %rdx
	xorq	%r10, %rsi
	movq	112(%rsp), %rax
	movq	120(%rsp), %rdi
	addq	%rax, %rdx
	adcq	%rdi, %rsi
	movq	%rdx, (%rcx)
	movq	%rsi, 8(%rcx)
	movq	$0, %rax
	movq	208(%rsp), %rbx
	movq	216(%rsp), %rbp
	movq	224(%rsp), %r12
	movq	232(%rsp), %r13
	movq	240(%rsp), %r14
	movq	248(%rsp), %r15
	movq	256(%rsp), %rsp
	ret 
L_chacha_ref_ic$1:
	movq	%rax, 8(%rsp)
	movq	%rcx, 16(%rsp)
	movl	$1634760805, 24(%rsp)
	movl	$857760878, 28(%rsp)
	movl	$2036477234, 32(%rsp)
	movl	$1797285236, 36(%rsp)
	movl	(%rdx), %eax
	movl	%eax, 40(%rsp)
	movl	4(%rdx), %eax
	movl	%eax, 44(%rsp)
	movl	8(%rdx), %eax
	movl	%eax, 48(%rsp)
	movl	12(%rdx), %eax
	movl	%eax, 52(%rsp)
	movl	16(%rdx), %eax
	movl	%eax, 56(%rsp)
	movl	20(%rdx), %eax
	movl	%eax, 60(%rsp)
	movl	24(%rdx), %eax
	movl	%eax, 64(%rsp)
	movl	28(%rdx), %eax
	movl	%eax, 68(%rsp)
	movl	%esi, 72(%rsp)
	movl	(%rdi), %eax
	movl	%eax, 76(%rsp)
	movl	4(%rdi), %eax
	movl	%eax, 80(%rsp)
	movl	8(%rdi), %eax
	movl	%eax, 84(%rsp)
	jmp 	L_chacha_ref_ic$8
L_chacha_ref_ic$9:
	movl	84(%rsp), %eax
	movl	%eax, 88(%rsp)
	movl	24(%rsp), %eax
	movl	28(%rsp), %ecx
	movl	32(%rsp), %edx
	movl	36(%rsp), %esi
	movl	40(%rsp), %edi
	movl	44(%rsp), %r8d
	movl	48(%rsp), %r9d
	movl	52(%rsp), %r10d
	movl	56(%rsp), %r11d
	movl	60(%rsp), %ebx
	movl	64(%rsp), %ebp
	movl	68(%rsp), %r12d
	movl	72(%rsp), %r13d
	movl	76(%rsp), %r14d
	movl	80(%rsp), %r15d
	movl	%r15d, 92(%rsp)
	movl	$10, %r15d
L_chacha_ref_ic$10:
	movl	%r15d, 96(%rsp)
	movl	92(%rsp), %r15d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$12, %edi
	roll	$12, %r9d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$7, %edi
	roll	$7, %r9d
	movl	%r15d, 100(%rsp)
	movl	88(%rsp), %r15d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$12, %r8d
	roll	$12, %r10d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$7, %r8d
	roll	$7, %r10d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$12, %r9d
	roll	$12, %r8d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$7, %r9d
	roll	$7, %r8d
	movl	%r15d, 88(%rsp)
	movl	100(%rsp), %r15d
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$12, %r10d
	roll	$12, %edi
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$7, %r10d
	roll	$7, %edi
	movl	%r15d, 92(%rsp)
	movl	96(%rsp), %r15d
	decl	%r15d
	cmpl	$0, %r15d
	jnbe	L_chacha_ref_ic$10
	movl	92(%rsp), %r15d
	addl	28(%rsp), %ecx
	addl	24(%rsp), %eax
	movl	%ecx, %ecx
	shlq	$32, %rcx
	movl	%eax, %eax
	xorq	%rax, %rcx
	addl	36(%rsp), %esi
	addl	32(%rsp), %edx
	movl	%esi, %eax
	shlq	$32, %rax
	movl	%edx, %edx
	xorq	%rdx, %rax
	movq	8(%rsp), %rdx
	movq	%rcx, (%rdx)
	addl	44(%rsp), %r8d
	addl	40(%rsp), %edi
	movl	%r8d, %ecx
	shlq	$32, %rcx
	movl	%edi, %esi
	xorq	%rsi, %rcx
	movq	%rax, 8(%rdx)
	addl	52(%rsp), %r10d
	addl	48(%rsp), %r9d
	movl	%r10d, %eax
	shlq	$32, %rax
	movl	%r9d, %esi
	xorq	%rsi, %rax
	movq	%rcx, 16(%rdx)
	addl	60(%rsp), %ebx
	addl	56(%rsp), %r11d
	movl	%ebx, %ecx
	shlq	$32, %rcx
	movl	%r11d, %esi
	xorq	%rsi, %rcx
	movq	%rax, 24(%rdx)
	addl	68(%rsp), %r12d
	addl	64(%rsp), %ebp
	movl	%r12d, %eax
	shlq	$32, %rax
	movl	%ebp, %esi
	xorq	%rsi, %rax
	movq	%rcx, 32(%rdx)
	addl	76(%rsp), %r14d
	addl	72(%rsp), %r13d
	movl	%r14d, %ecx
	shlq	$32, %rcx
	movl	%r13d, %esi
	xorq	%rsi, %rcx
	movq	%rax, 40(%rdx)
	movl	88(%rsp), %eax
	addl	84(%rsp), %eax
	addl	80(%rsp), %r15d
	movl	%eax, %eax
	shlq	$32, %rax
	movl	%r15d, %esi
	xorq	%rsi, %rax
	movq	%rcx, 48(%rdx)
	movq	%rax, 56(%rdx)
	movq	16(%rsp), %rax
	addq	$64, %rdx
	addq	$-64, %rax
	movq	%rdx, 8(%rsp)
	movq	%rax, 16(%rsp)
	movl	72(%rsp), %eax
	incl	%eax
	movl	%eax, 72(%rsp)
L_chacha_ref_ic$8:
	movq	16(%rsp), %rax
	cmpq	$64, %rax
	jnb 	L_chacha_ref_ic$9
	cmpq	$0, %rax
	jbe 	L_chacha_ref_ic$2
	movl	84(%rsp), %eax
	movl	%eax, 88(%rsp)
	movl	24(%rsp), %eax
	movl	28(%rsp), %ecx
	movl	32(%rsp), %edx
	movl	36(%rsp), %esi
	movl	40(%rsp), %edi
	movl	44(%rsp), %r8d
	movl	48(%rsp), %r9d
	movl	52(%rsp), %r10d
	movl	56(%rsp), %r11d
	movl	60(%rsp), %ebx
	movl	64(%rsp), %ebp
	movl	68(%rsp), %r12d
	movl	72(%rsp), %r13d
	movl	76(%rsp), %r14d
	movl	80(%rsp), %r15d
	movl	%r15d, 92(%rsp)
	movl	$10, %r15d
L_chacha_ref_ic$7:
	movl	%r15d, 96(%rsp)
	movl	92(%rsp), %r15d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$12, %edi
	roll	$12, %r9d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$7, %edi
	roll	$7, %r9d
	movl	%r15d, 100(%rsp)
	movl	88(%rsp), %r15d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$12, %r8d
	roll	$12, %r10d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$7, %r8d
	roll	$7, %r10d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$12, %r9d
	roll	$12, %r8d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$7, %r9d
	roll	$7, %r8d
	movl	%r15d, 88(%rsp)
	movl	100(%rsp), %r15d
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$12, %r10d
	roll	$12, %edi
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$7, %r10d
	roll	$7, %edi
	movl	%r15d, 92(%rsp)
	movl	96(%rsp), %r15d
	decl	%r15d
	cmpl	$0, %r15d
	jnbe	L_chacha_ref_ic$7
	movl	92(%rsp), %r15d
	addl	24(%rsp), %eax
	addl	28(%rsp), %ecx
	addl	32(%rsp), %edx
	addl	36(%rsp), %esi
	addl	40(%rsp), %edi
	addl	44(%rsp), %r8d
	addl	48(%rsp), %r9d
	addl	52(%rsp), %r10d
	addl	56(%rsp), %r11d
	addl	60(%rsp), %ebx
	addl	64(%rsp), %ebp
	addl	68(%rsp), %r12d
	addl	72(%rsp), %r13d
	addl	76(%rsp), %r14d
	addl	80(%rsp), %r15d
	movl	%r15d, 92(%rsp)
	movl	88(%rsp), %r15d
	addl	84(%rsp), %r15d
	movl	%r15d, 88(%rsp)
	movl	92(%rsp), %r15d
	movl	%eax, 24(%rsp)
	movl	%ecx, 28(%rsp)
	movl	%edx, 32(%rsp)
	movl	%esi, 36(%rsp)
	movl	%edi, 40(%rsp)
	movl	%r8d, 44(%rsp)
	movl	%r9d, 48(%rsp)
	movl	%r10d, 52(%rsp)
	movl	%r11d, 56(%rsp)
	movl	%ebx, 60(%rsp)
	movl	%ebp, 64(%rsp)
	movl	%r12d, 68(%rsp)
	movl	%r13d, 72(%rsp)
	movl	%r14d, 76(%rsp)
	movl	%r15d, 80(%rsp)
	movl	88(%rsp), %eax
	movl	%eax, 84(%rsp)
	movq	8(%rsp), %rax
	movq	16(%rsp), %rcx
	movq	%rcx, %rdx
	shrq	$3, %rdx
	movq	$0, %rsi
	jmp 	L_chacha_ref_ic$5
L_chacha_ref_ic$6:
	movq	24(%rsp,%rsi,8), %rdi
	movq	%rdi, (%rax,%rsi,8)
	incq	%rsi
L_chacha_ref_ic$5:
	cmpq	%rdx, %rsi
	jb  	L_chacha_ref_ic$6
	shlq	$3, %rsi
	jmp 	L_chacha_ref_ic$3
L_chacha_ref_ic$4:
	movb	24(%rsp,%rsi), %dl
	movb	%dl, (%rax,%rsi)
	incq	%rsi
L_chacha_ref_ic$3:
	cmpq	%rcx, %rsi
	jb  	L_chacha_ref_ic$4
L_chacha_ref_ic$2:
	ret 
L_chacha_xor_ref_ic$1:
	movq	%rax, 8(%rsp)
	movq	%rcx, 16(%rsp)
	movq	%rdx, 24(%rsp)
	movl	$1634760805, 32(%rsp)
	movl	$857760878, 36(%rsp)
	movl	$2036477234, 40(%rsp)
	movl	$1797285236, 44(%rsp)
	movl	(%rsi), %eax
	movl	%eax, 48(%rsp)
	movl	4(%rsi), %eax
	movl	%eax, 52(%rsp)
	movl	8(%rsi), %eax
	movl	%eax, 56(%rsp)
	movl	12(%rsi), %eax
	movl	%eax, 60(%rsp)
	movl	16(%rsi), %eax
	movl	%eax, 64(%rsp)
	movl	20(%rsi), %eax
	movl	%eax, 68(%rsp)
	movl	24(%rsi), %eax
	movl	%eax, 72(%rsp)
	movl	28(%rsi), %eax
	movl	%eax, 76(%rsp)
	movl	%edi, 80(%rsp)
	movl	(%r8), %eax
	movl	%eax, 84(%rsp)
	movl	4(%r8), %eax
	movl	%eax, 88(%rsp)
	movl	8(%r8), %eax
	movl	%eax, 92(%rsp)
	jmp 	L_chacha_xor_ref_ic$8
L_chacha_xor_ref_ic$9:
	movl	92(%rsp), %eax
	movl	%eax, 96(%rsp)
	movl	32(%rsp), %eax
	movl	36(%rsp), %ecx
	movl	40(%rsp), %edx
	movl	44(%rsp), %esi
	movl	48(%rsp), %edi
	movl	52(%rsp), %r8d
	movl	56(%rsp), %r9d
	movl	60(%rsp), %r10d
	movl	64(%rsp), %r11d
	movl	68(%rsp), %ebx
	movl	72(%rsp), %ebp
	movl	76(%rsp), %r12d
	movl	80(%rsp), %r13d
	movl	84(%rsp), %r14d
	movl	88(%rsp), %r15d
	movl	%r15d, 100(%rsp)
	movl	$10, %r15d
L_chacha_xor_ref_ic$10:
	movl	%r15d, 104(%rsp)
	movl	100(%rsp), %r15d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$12, %edi
	roll	$12, %r9d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$7, %edi
	roll	$7, %r9d
	movl	%r15d, 108(%rsp)
	movl	96(%rsp), %r15d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$12, %r8d
	roll	$12, %r10d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$7, %r8d
	roll	$7, %r10d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$12, %r9d
	roll	$12, %r8d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$7, %r9d
	roll	$7, %r8d
	movl	%r15d, 96(%rsp)
	movl	108(%rsp), %r15d
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$12, %r10d
	roll	$12, %edi
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$7, %r10d
	roll	$7, %edi
	movl	%r15d, 100(%rsp)
	movl	104(%rsp), %r15d
	decl	%r15d
	cmpl	$0, %r15d
	jnbe	L_chacha_xor_ref_ic$10
	movl	100(%rsp), %r15d
	addl	36(%rsp), %ecx
	addl	32(%rsp), %eax
	movl	%ecx, %ecx
	shlq	$32, %rcx
	movl	%eax, %eax
	xorq	%rax, %rcx
	movq	16(%rsp), %rax
	xorq	(%rax), %rcx
	addl	44(%rsp), %esi
	addl	40(%rsp), %edx
	movl	%esi, %esi
	shlq	$32, %rsi
	movl	%edx, %edx
	xorq	%rdx, %rsi
	xorq	8(%rax), %rsi
	movq	8(%rsp), %rdx
	movq	%rcx, (%rdx)
	addl	52(%rsp), %r8d
	addl	48(%rsp), %edi
	movl	%r8d, %ecx
	shlq	$32, %rcx
	movl	%edi, %edi
	xorq	%rdi, %rcx
	xorq	16(%rax), %rcx
	movq	%rsi, 8(%rdx)
	addl	60(%rsp), %r10d
	addl	56(%rsp), %r9d
	movl	%r10d, %esi
	shlq	$32, %rsi
	movl	%r9d, %edi
	xorq	%rdi, %rsi
	xorq	24(%rax), %rsi
	movq	%rcx, 16(%rdx)
	addl	68(%rsp), %ebx
	addl	64(%rsp), %r11d
	movl	%ebx, %ecx
	shlq	$32, %rcx
	movl	%r11d, %edi
	xorq	%rdi, %rcx
	xorq	32(%rax), %rcx
	movq	%rsi, 24(%rdx)
	addl	76(%rsp), %r12d
	addl	72(%rsp), %ebp
	movl	%r12d, %esi
	shlq	$32, %rsi
	movl	%ebp, %edi
	xorq	%rdi, %rsi
	xorq	40(%rax), %rsi
	movq	%rcx, 32(%rdx)
	addl	84(%rsp), %r14d
	addl	80(%rsp), %r13d
	movl	%r14d, %ecx
	shlq	$32, %rcx
	movl	%r13d, %edi
	xorq	%rdi, %rcx
	xorq	48(%rax), %rcx
	movq	%rsi, 40(%rdx)
	movl	96(%rsp), %esi
	addl	92(%rsp), %esi
	addl	88(%rsp), %r15d
	movl	%esi, %esi
	shlq	$32, %rsi
	movl	%r15d, %edi
	xorq	%rdi, %rsi
	xorq	56(%rax), %rsi
	movq	%rcx, 48(%rdx)
	movq	%rsi, 56(%rdx)
	movq	24(%rsp), %rcx
	addq	$64, %rdx
	addq	$64, %rax
	addq	$-64, %rcx
	movq	%rdx, 8(%rsp)
	movq	%rax, 16(%rsp)
	movq	%rcx, 24(%rsp)
	movl	80(%rsp), %eax
	incl	%eax
	movl	%eax, 80(%rsp)
L_chacha_xor_ref_ic$8:
	movq	24(%rsp), %rax
	cmpq	$64, %rax
	jnb 	L_chacha_xor_ref_ic$9
	cmpq	$0, %rax
	jbe 	L_chacha_xor_ref_ic$2
	movl	92(%rsp), %eax
	movl	%eax, 96(%rsp)
	movl	32(%rsp), %eax
	movl	36(%rsp), %ecx
	movl	40(%rsp), %edx
	movl	44(%rsp), %esi
	movl	48(%rsp), %edi
	movl	52(%rsp), %r8d
	movl	56(%rsp), %r9d
	movl	60(%rsp), %r10d
	movl	64(%rsp), %r11d
	movl	68(%rsp), %ebx
	movl	72(%rsp), %ebp
	movl	76(%rsp), %r12d
	movl	80(%rsp), %r13d
	movl	84(%rsp), %r14d
	movl	88(%rsp), %r15d
	movl	%r15d, 100(%rsp)
	movl	$10, %r15d
L_chacha_xor_ref_ic$7:
	movl	%r15d, 104(%rsp)
	movl	100(%rsp), %r15d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$12, %edi
	roll	$12, %r9d
	addl	%edi, %eax
	addl	%r9d, %edx
	xorl	%eax, %r13d
	xorl	%edx, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r11d
	addl	%r15d, %ebp
	xorl	%r11d, %edi
	xorl	%ebp, %r9d
	roll	$7, %edi
	roll	$7, %r9d
	movl	%r15d, 108(%rsp)
	movl	96(%rsp), %r15d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$12, %r8d
	roll	$12, %r10d
	addl	%r8d, %ecx
	addl	%r10d, %esi
	xorl	%ecx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %ebx
	addl	%r15d, %r12d
	xorl	%ebx, %r8d
	xorl	%r12d, %r10d
	roll	$7, %r8d
	roll	$7, %r10d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$16, %r13d
	roll	$16, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$12, %r9d
	roll	$12, %r8d
	addl	%r9d, %ecx
	addl	%r8d, %eax
	xorl	%ecx, %r13d
	xorl	%eax, %r15d
	roll	$8, %r13d
	roll	$8, %r15d
	addl	%r13d, %r12d
	addl	%r15d, %ebp
	xorl	%r12d, %r9d
	xorl	%ebp, %r8d
	roll	$7, %r9d
	roll	$7, %r8d
	movl	%r15d, 96(%rsp)
	movl	108(%rsp), %r15d
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$16, %r14d
	roll	$16, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$12, %r10d
	roll	$12, %edi
	addl	%r10d, %edx
	addl	%edi, %esi
	xorl	%edx, %r14d
	xorl	%esi, %r15d
	roll	$8, %r14d
	roll	$8, %r15d
	addl	%r14d, %r11d
	addl	%r15d, %ebx
	xorl	%r11d, %r10d
	xorl	%ebx, %edi
	roll	$7, %r10d
	roll	$7, %edi
	movl	%r15d, 100(%rsp)
	movl	104(%rsp), %r15d
	decl	%r15d
	cmpl	$0, %r15d
	jnbe	L_chacha_xor_ref_ic$7
	movl	100(%rsp), %r15d
	addl	32(%rsp), %eax
	addl	36(%rsp), %ecx
	addl	40(%rsp), %edx
	addl	44(%rsp), %esi
	addl	48(%rsp), %edi
	addl	52(%rsp), %r8d
	addl	56(%rsp), %r9d
	addl	60(%rsp), %r10d
	addl	64(%rsp), %r11d
	addl	68(%rsp), %ebx
	addl	72(%rsp), %ebp
	addl	76(%rsp), %r12d
	addl	80(%rsp), %r13d
	addl	84(%rsp), %r14d
	addl	88(%rsp), %r15d
	movl	%r15d, 100(%rsp)
	movl	96(%rsp), %r15d
	addl	92(%rsp), %r15d
	movl	%r15d, 96(%rsp)
	movl	100(%rsp), %r15d
	movl	%eax, 32(%rsp)
	movl	%ecx, 36(%rsp)
	movl	%edx, 40(%rsp)
	movl	%esi, 44(%rsp)
	movl	%edi, 48(%rsp)
	movl	%r8d, 52(%rsp)
	movl	%r9d, 56(%rsp)
	movl	%r10d, 60(%rsp)
	movl	%r11d, 64(%rsp)
	movl	%ebx, 68(%rsp)
	movl	%ebp, 72(%rsp)
	movl	%r12d, 76(%rsp)
	movl	%r13d, 80(%rsp)
	movl	%r14d, 84(%rsp)
	movl	%r15d, 88(%rsp)
	movl	96(%rsp), %eax
	movl	%eax, 92(%rsp)
	movq	8(%rsp), %rax
	movq	16(%rsp), %rcx
	movq	24(%rsp), %rdx
	movq	%rdx, %rsi
	shrq	$3, %rsi
	movq	$0, %rdi
	jmp 	L_chacha_xor_ref_ic$5
L_chacha_xor_ref_ic$6:
	movq	(%rcx,%rdi,8), %r8
	xorq	32(%rsp,%rdi,8), %r8
	movq	%r8, (%rax,%rdi,8)
	incq	%rdi
L_chacha_xor_ref_ic$5:
	cmpq	%rsi, %rdi
	jb  	L_chacha_xor_ref_ic$6
	shlq	$3, %rdi
	jmp 	L_chacha_xor_ref_ic$3
L_chacha_xor_ref_ic$4:
	movb	(%rcx,%rdi), %sil
	xorb	32(%rsp,%rdi), %sil
	movb	%sil, (%rax,%rdi)
	incq	%rdi
L_chacha_xor_ref_ic$3:
	cmpq	%rdx, %rdi
	jb  	L_chacha_xor_ref_ic$4
L_chacha_xor_ref_ic$2:
	ret 
